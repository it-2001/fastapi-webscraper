[{"name": "Proxmox Azure OID", "date": "17.2.2023", "author": "admin", "url": "https://blog.iservery.com/2023/02/17/proxmox-azure-oid/", "content": "\nProxmox OIDC Authentication Azure AD\n\nZdroj\nhttps://www.reddit.com/r/Proxmox/comments/pqxu2o/proxmox_oidc_authentication_azure_ad/\nDobr\u00fd den, ned\u00e1vno jsem upgradoval sv\u016fj Proxmox Server na 7.0 a t\u00edm jsem z\u00edskal nov\u00fd autentiza\u010dn\u00ed mechanismus OIDC!!!\u00a0Po prohled\u00e1n\u00ed webu jsem nena\u0161el \u017e\u00e1dnou dokumentaci o tom, jak to nastavit pomoc\u00ed Azure AD, tak\u017ee te\u010f, kdy\u017e m\u00e1m sv\u00e9 nastaven\u00ed, jsem si myslel, \u017ee se pod\u011bl\u00edm.\nKrok 1 \u2013 P\u0159ihlaste se do Azure AD a klikn\u011bte na\u00a0Registrace aplikac\u00ed\u00a0.\n\nKrok 2 \u2013 Klikn\u011bte na\u00a0Nov\u00e1 registrace\u00a0a p\u0159idejte n\u00e1zev a jeden ze sv\u00fdch server\u016f Proxmox\n\nKrok 3 \u2013 P\u0159idejte v\u0161echny sv\u00e9 adresy URL pro servery Proxmox kliknut\u00edm na\u00a0P\u0159idat URI\u00a0a pot\u00e9 na Ulo\u017eit.\n\nKrok 4 \u2013 Klikn\u011bte na\u00a0Certificates & Secrets (Certifik\u00e1ty a tajemstv\u00ed)\u00a0a pot\u00e9 na\u00a0New Client Secret (Nov\u00fd tajn\u00fd kl\u00ed\u010d klienta),\u00a0kde m\u016f\u017eete ur\u010dit, kdy chcete, aby platnost tajemstv\u00ed vypr\u0161ela.\u00a0Ujist\u011bte se, \u017ee jste tajemstv\u00ed ulo\u017eili pod\u00a0Value,\u00a0budeme to pot\u0159ebovat pozd\u011bji.\n\nKrok 5 \u2013 Klikn\u011bte na\u00a0P\u0159ehled\u00a0Kop\u00edrovat\u00a0ID klienta\u00a0a pot\u00e9 klikn\u011bte na Koncov\u00e9 body\n\nZkop\u00edrujte odkaz na\u00a0dokument metadat OpenID Connect\u00a0a odstra\u0148te\u00a0/.well-known/openid-configuration\u00a0tuto \u010d\u00e1st z odkazu, tak\u017ee skon\u010d\u00edte s n\u011b\u010d\u00edm takov\u00fdmhttps://login.microsoftonline.com/{Va\u0161e\u00a0ID tenanta}/v2.0\n\nKrok 6 \u2013 P\u0159ejd\u011bte na Proxmox and\u00a0Authentication\u00a0\u2013\u00a0Add\u00a0\u2013\u00a0OpenID Connect\u00a0a pot\u00e9 p\u0159idejte hodnoty pro Azure AD\n\nNyn\u00ed se odhlaste a p\u0159ihlaste se svou novou\u00a0sf\u00e9rou\u00a0a m\u011bli byste b\u00fdt p\u0159ipraveni.\u00a0To by m\u011blo b\u00fdt v podstat\u011b stejn\u00e9, pokud pou\u017e\u00edv\u00e1te Okta, ADFS nebo n\u011bco jin\u00e9ho.\u00a0Mysl\u00edm, \u017ee hlavn\u00ed v\u011bc, kterou pot\u0159ebujete v\u011bd\u011bt, je, \u017ee adresa URL vydavatele skute\u010dn\u011b hled\u00e1 va\u0161e metadata OpenID Connect a k adrese URL automaticky p\u0159ipojuje\u00a0/.well-known/openid-configuration\u00a0, tak\u017ee je nemus\u00edte p\u0159id\u00e1vat znovu.\n"}, {"name": "second dhcp + drbl", "date": "10.1.2023", "author": "admin", "url": "https://blog.iservery.com/2023/01/10/second-dhcp-drbl/", "content": "\nHi Steven!Will try give details of my steps.Configuration:\u2013 central server \u201eRain\u201c (DHCP) connected to 12 subnets (192.168.10.x \u2013 192.168.120.x);\u2013 server \u201eDrbl\u201c (Clonezilla) connected to same subnets;\u2013 from 3 to 25 computers in each subnet;\n==Part of dhcpd.conf from Rain===server-identifier rain.ctd.loc;option domain-name \u201ectd.loc\u201c;option domain-name-servers ns.ctd.loc;\nsubnet 192.168.10.0 netmask 255.255.255.0 {\u00a0 authoritative;\u00a0 option routers\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 192.168.10.100;\u00a0 option subnet-mask\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 255.255.255.0;\u00a0 option broadcast-address\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 192.168.10.255;\u00a0 option domain-name-servers\u00a0\u00a0\u00a0\u00a0\u00a0 192.168.10.100;\u00a0 option netbios-name-servers\u00a0\u00a0\u00a0\u00a0 192.168.10.100;\u00a0 host pc10-1 {\u00a0\u00a0 hardware ethernet 00:aa:aa:aa:aa:03;\u00a0\u00a0 fixed-address 192.168.10.1;\u00a0\u00a0 }\u00a0 host pc10-2 {\u00a0\u00a0 hardware ethernet 00:aa:aa:aa:aa:98;\u00a0\u00a0 fixed-address 192.168.10.2;\u00a0\u00a0 }\n\u2026 and so on for all subnets\u2026\n\u00a0 host pc12-25 {\u00a0\u00a0 hardware ethernet 00:ff:ff:ff:ff:45;\u00a0\u00a0 fixed-address 192.168.120.25;\u00a0\u00a0 }}===end of dhcpd.conf===\n1) Do steps from 1 to 3a by this:\u00a0http://drbl.sourceforge.net/one4all\u00a0;2) Copy dhcpd.conf from Rain, parse it with \u201e/opt/drbl/bin/parse_dhcpd_conf\u201c;3) Collect parsed MAC addresses and put them into /etc/drbl/macadr-eth0.1.txt, macadr-eth0.2.txt, etc;4) Collect parsed names/IP\u2019s and:\u00a0 a) put them all to /opt/drbl/conf/client-ip-hostname\u00a0 b) put them to /etc/drbl/IP-grp-AA, IP-grp-BB, etc by subnets5) Create \u201edummy\u201c interface for \u201eWAN\u201c purpose (my Drbl has no internet connection);6) Run \u201e/opt/drbl/sbin/drblpush -i\u201c and used already collected MAC\u2019s in macadr-eth0.*.txt files;7) Run \u201e/opt/drbl/sbin/mknic-nbi \u2013udhcpc-port 1068\u201c;8) Run two dhcpd instances:\u00a0 a) /usr/sbin/dhcpd -q -pf /var/run/dhcpd67.pid\u00a0 b) /usr/sbin/dhcpd -q -pf /var/run/dhcpd1067.pid -p 1067\n=== Can you suggest a way to modify /etc/init.d/dhcpd script to achieve this? ===\n9) Run /opt/drbl/sbin/dcs and start enjoying =)\nNow client work as follow:\u2013 power on, PXE ROM in network card (pxeclient) asks DHCP with ip, subnet, filename, next-server options;\u2013 Rain answers with ip and subnet options (pxeclient ignores);\u2013 Drbl answers with ip, subnet, filename, next-server options (pxeclient accepts);\u2013 pxeclient receive menu, then receive linux kernel and vfs file via tftp;\u2013 linux booting in client memory, now it linuxclient \ud83d\ude42 ;\u2013 linuxclient asks ip, subnet, next-server on 1067 port, wait answer to 1068 port from \u201edrbl\u201c;\u2013 Drbl answers from 1067 to clients 1068 port (linuxclient accepts);\u2013 cloning or deploying;\u2013 w1nd0ws booting and asks for ip and subnet (winclient);\u2013 Rain or Drbl (who faster) answers (winclient accepts).\nAs result my DRBL server works in current environment.\n=====================================================================Please, let me know, if you find \u201eproxy DHCP\u201c daemon for Linux! Because with it, life would be much easier (there are only one dhcpd.conf with \u201ehost\u201c entries)! =)\nClient work WITH PROXY DHCP:\u2013 power on, PXE ROM in network card (pxeclient) asks DHCP with ip, subnet, filename, next-server options;\u2013 Rain answers with ip and subnet options (pxeclient ignores);\u2013 proxyDHCP forwards the same Rain\u2019s answer and add to it filename and next-server options (pxeclient accepts);\u2013 pxeclient receive menu, then receive linux kernel and vfs file via tftp;\u2013 linux booting in client memory, now it linuxclient :-);\u2013 linuxclient asks ip, subnet, next-server on 67/68 port, wait answer from any server (/opt/drbl/sbin/mknic-nbi \u2013check-server-name n);\u2013 proxyDHCP forwards usual Rain\u2019s answer and add to it filename option (linuxclient accepts);\u2013 cloning or deploying;\u2013 w1nd0ws booting and asks for ip and subnet (winclient);\u2013 Rain answers (winclient accepts).=====================================================================\nThank you for cooperate!\n"}, {"name": "T\u011bstoviny 100krat jinak \ud83d\ude04", "date": "16.4.2022", "author": "admin", "url": "https://blog.iservery.com/2022/04/16/testoviny-100krat-jinak-%f0%9f%98%84/", "content": "\n\nhttp://cuocadolce.blogspot.com/2020/03/5-rychlych-omacek-na-testoviny.html?m=1\n\n"}, {"name": "T\u011bstoviny se zelen\u00fdm hr\u00e1\u0161kem", "date": "16.4.2022", "author": "admin", "url": "https://blog.iservery.com/2022/04/16/testoviny-se-zelenym-hraskem/", "content": "\n\nhttps://www.washingtonpost.com/food/2022/04/14/pasta-with-smothered-peas-recipe/\n\n\n"}, {"name": "Zaj\u00edmav\u00e9 roz\u0161\u00ed\u0159en\u00ed PROXMOXu 7..", "date": "4.4.2022", "author": "admin", "url": "https://blog.iservery.com/2022/04/04/zajimave-rozsireni-proxmoxu-7/", "content": "\nhttps://github.com/ivanhao/pvetools\n"}, {"name": "Pasta cacio e pepe", "date": "1.4.2022", "author": "admin", "url": "https://blog.iservery.com/2022/04/01/pasta-cacio-epasta-cacio-e-pepe-pepe/", "content": "\n\nPasta cacio e pepe\n\nPasta cacio e pepe jsou tradi\u010dn\u00ed \u0159\u00edmsk\u00e9\u00a0t\u011bstoviny: pecorino a \u010dern\u00fd pep\u0159, jejich\u017e p\u0159\u00edprava se zd\u00e1 jednoduch\u00e1, ale nen\u00ed. Mus\u00edte um\u011bt vytvo\u0159it om\u00e1\u010dku, kter\u00e1 je ve spr\u00e1vn\u00e9m m\u00edst\u011b kr\u00e9mov\u00e1 a bohu\u017eel chyba je hned za rohem, sta\u010d\u00ed m\u00e1lo, abyste se ocitli s hromadou hrudek. Ale pokud budete \u00fasp\u011b\u0161n\u00ed, m\u016f\u017eete nepochybn\u011b usp\u011bt , proto\u017ee k z\u00edsk\u00e1n\u00ed t\u011bstovin bude sta\u010dit pou\u017e\u00edt n\u011bkolik mal\u00fdch trik\u016f. P\u0159edev\u0161\u00edm v\u00fdb\u011br s\u00fdra, ne ledajak\u00e9ho pecorina, ale v\u00fdhradn\u011b pecorino romano. Voda v t\u011bstovin\u00e1ch by m\u011bla b\u00fdt m\u00e1lo osolen\u00e1, proto\u017ee pecorino je ji\u017e velmi slan\u00e9. \u010cern\u00e9ho pep\u0159e p\u0159idejte hodn\u011b. Neptejte se m\u011b, \u010d\u00edm to m\u016f\u017eete nahradit nebo jestli to m\u016f\u017eete odstranit, proto\u017ee to nejde. P\u0159ipravujeme tradi\u010dn\u00ed recepturu a p\u0159ipravujeme cacio e pepe. Pep\u0159 to bere. Pokud nem\u00e1te r\u00e1di pep\u0159, m\u016f\u017eete si ud\u011blat jin\u00fd recept. K prom\u00edch\u00e1n\u00ed om\u00e1\u010dky pou\u017eijte vodu z va\u0159en\u00ed t\u011bstovin, proto\u017ee \u0161krob uvoln\u011bn\u00fd z t\u011bstovin do vody zabr\u00e1n\u00ed sr\u00e1\u017een\u00ed b\u00edlkovin. Jakou pastu pou\u017e\u00edt? Bucatini, \u0161pagety, pici, tonnarelli, \u0161pagety alla chitarra, pokud se jedn\u00e1 o \u010derstv\u00e9 t\u011bstoviny. Pro m\u00e9 cacio e pepe jsem si vybral dom\u00e1c\u00ed tagliatelle. Nyn\u00ed jsme p\u0159ipraveni p\u0159ipravit t\u011bstoviny cacio e pepe a um\u011bt si vytvo\u0159it om\u00e1\u010dku, kter\u00e1 je ve spr\u00e1vn\u00e9m m\u00edst\u011b kr\u00e9mov\u00e1 a bohu\u017eel chyba je hned za rohem, sta\u010d\u00ed m\u00e1lo, abyste se ocitli s hromadou hrudek !!!\u00a0Ale pokud budete \u00fasp\u011b\u0161n\u00ed, m\u016f\u017eete nepochybn\u011b zvit\u011bzit i vy, proto\u017ee k z\u00edsk\u00e1n\u00ed\u00a0t\u011bstovin s cacio e pepe bude sta\u010dit n\u011bkolik mal\u00fdch trik\u016f.\u00a0\n\nObt\u00ed\u017enost N\u00edzk\u00e1\u010cas na p\u0159\u00edpravu 10 minut\u010cas na va\u0159en\u00ed 10 minutPorce 4 osobyZp\u016fsob va\u0159en\u00ed Spor\u00e1kKuchyn\u011b italsk\u00e1\nIngredience\n320\u00a0g\u00a0t\u011bstovin200\u00a0g\u00a0Pecorino Romano\u00a0(st\u0159edn\u011b vyzr\u00e1l\u00e9 a nastrouhan\u00e9)ochutnat\u00a0\u010cern\u00fd pep\u0159 v zrnechochutnat\u00a0s\u016fl\nIngredience\n320\u00a0g\u00a0t\u011bstovin200\u00a0g\u00a0Pecorino Romano\u00a0(st\u0159edn\u011b vyzr\u00e1l\u00e9 a nastrouhan\u00e9)ochutnat\u00a0\u010cern\u00fd pep\u0159 v zrnechochutnat\u00a0s\u016fl\nP\u0159\u00edpravaPecorino romano nastrouh\u00e1me na jemno !!!. Vodu na t\u011bstoviny p\u0159ive\u010fte k varu (budete muset pou\u017e\u00edt asi polovinu toho, co obvykle pou\u017e\u00edv\u00e1te, aby byla bohat\u0161\u00ed na \u0161krob). Dochut\u00edme m\u00edrn\u011b sol\u00ed a uva\u0159\u00edme t\u011bstoviny. Zrnka pep\u0159e rozma\u010dkejte pali\u010dkou, polovinu nasypte do p\u00e1nve a na m\u00edrn\u00e9m ohni nechte op\u00e9ct, pot\u00e9 p\u0159idejte dv\u011b nab\u011bra\u010dky vody na va\u0159en\u00ed. T\u011bstoviny sce\u010fte ve stavu \u201eal dente\u201c trochu pevn\u00e9, p\u0159endejte je na p\u00e1nev s pep\u0159em, za st\u00e1l\u00e9ho m\u00edch\u00e1n\u00ed p\u0159ive\u010fte k varu p\u0159id\u00e1n\u00edm jedn\u00e9 nebo dvou nab\u011bra\u010dek va\u0159\u00edc\u00ed vody podle pot\u0159eby (jakmile oschnou).\nP\u0159ipravte si kr\u00e9m pecorino romano: je d\u016fle\u017eit\u00e9 to ud\u011blat na posledn\u00ed chv\u00edli, odolejte poku\u0161en\u00ed p\u0159ipravit si ho p\u0159edem, jinak p\u0159\u00edli\u0161 ztuhne. Do m\u00edsy p\u0159endejte sto gram\u016f nastrouhan\u00e9ho s\u00fdra pecorino, p\u0159idejte nab\u011bra\u010dku va\u0159\u00edc\u00ed vody. D\u016fkladn\u011b prom\u00edch\u00e1me metli\u010dkou a po tro\u0161k\u00e1ch p\u0159id\u00e1me dal\u0161\u00ed vodu a pot\u00e9 zbytek pecorina (trochu si nech\u00e1me stranou na dochucen\u00ed na z\u00e1v\u011br) v\u017edy prom\u00edch\u00e1me metli\u010dkou, abychom z\u00edskali hladk\u00fd kr\u00e9m bez hrudek.\nP\u0159ed p\u0159id\u00e1n\u00edm pecorino smetany do t\u011bstovin polo\u017ete misku, kter\u00e1 ji obsahuje, na p\u00e1nev s je\u0161t\u011b horkou vodou na va\u0159en\u00ed t\u011bstovin a znovu prom\u00edchejte metli\u010dkou (tento krok je nutn\u00fd, aby se smetana pecorino trochu zah\u0159\u00e1la a m\u011bla podobnou teplotu k t\u011bstovin\u00e1m).\nVypn\u011bte ohe\u0148 pod p\u00e1nv\u00ed na t\u011bstoviny, za st\u00e1l\u00e9ho m\u00edch\u00e1n\u00ed kuchy\u0148sk\u00fdmi kle\u0161t\u011bmi vlijte smetanu pecorino. Dochu\u0165te odlo\u017een\u00fdm pecorinem, znovu prom\u00edchejte a ihned pod\u00e1vejte.\nPozn\u00e1mkaP\u0159i realizaci receptu jsem postupoval podle n\u00e1vodu Giallo Zafferana .\n"}, {"name": "MIKROTIK VLAN SWITCH", "date": "4.1.2022", "author": "admin", "url": "https://blog.iservery.com/2022/01/04/mikrotik-vlan-switch/", "content": "\nhttps://www.derekseaman.com/2021/03/home-lab-mikrotik-ccr2004-and-crs317-configuration.html\nVery recently I decided to completely re-do my home lab hardware, and really step things up a few notches. My previous home lab gear was too old to use for anything useful, so out with old and in with the new! As a result of this new gear, I wanted to publish at least one blog post on the network configuration since it\u2019s no longer just a simple flat network. The hardware will support both VMware and Nutanix hypervisors, including SDN solutions like NSX-T. This post will focus on the network, and configuring my MikroTik switch and router.\nMy single server for nested virtualization is:\nDell Precision 7920 TowerDual Intel Gold 5220R CPUs (2.2 GHz, 48 total pCores, 96 threads)384GB DDR4-2666 ECC RAM5TB NVMe local storage (2x 2TB, 1x 1TB) via MegaRAID 9460-16i4 port 10Gbps Marvell 41000 SFP+ NIC (via HPE)\nFor my lab network gear, I have:\nMikroTik\u00a0CRS317-1G-16S+RM\u00a0(16x 10 Gbps SFP+ ports) \u2013 Layer 2 switchMikroTik\u00a0CCR2004-1G-12S+2XS\u00a0(12 10 Gbps SFP+ ports) \u2013 Router\nAs you can see, this is a pretty beefy lab with one decked out workstation for virtualization, and upstream 10Gb switching and routing. While the Dell Precision 7920 is not on the VMware HCL, the NIC and RAID card are. So I had no issues installing ESXi 7.0 U2, and didn\u2019t have to manually install any driver VIBs.\nBecause I\u2019ll be building up and tearing down nested environments on a regular basis, I wanted all lab gear on their own networks, routed with my home LAN and internet. I set aside 172.16.0.0/12 for the lab networks. My home LAN is on the 10.13.2.0/24 network. The diagram below shows the physical topology of network. Note the NSX-T portion is still under construction, and more will come in the future on that.\nAs you can see, my workstation is configured with vSphere using three VLANs for traffic separation. I\u2019m not using vSAN, or that would be on yet another port group and VLAN. Two physical NICs are dedicated to a management distributed switch (left), and two will be used for NSX-T (right). The MikroTik CRS317 will handle all the layer 2 switching needs, while the CCR2004 will handle all routing for the network and out to the rest of the LAN. The purple 1G LAN switch is just an unmanaged switch that\u2019s in the room to support additional devices in the same room and serves as an uplink to the rest of the LAN.\u00a0 \u00a0\n\n\nMikroTik CRS317-1G-16S+RM Switch Config\nWhen looking at what network gear to use, I really wanted to go 10Gb end-to-end, to help future proof the network for the foreseeable future. Enterprise class 10Gb switches and routers are extremely pricey, and not in my home budget. However, with some performance cautions, the MikroTik gear supports 10Gb traffic and is extremely affordable. Would I use MikroTik in a true enterprise environment? Nope! But it\u2019s working well for a home lab with modest performance requirements.\u00a0\nFirst up, I wanted to configure the\u00a0CRS317-1G-16S+RM, strictly for layer 2 operation. And I wanted the 1Gbps management port on my 10.13.2.x network for out of band management of the 172.16.0.0 address space. If you reset your CRS317 to the default state and remove the default configuration, you can simply copy/paste all of the commands below and completely configure the switch without touching the UI.\u00a0\nNote that both the router and switch are configured for jumbo frames, so that I can deploy any SDN I wish.\n\t\t\t\t\n\t\t\t\t\t# Setup OOB management interface\n/interface ethernet set [ find default-name=ether1 ] name=MGMT\n/ip address\nadd address=10.13.2.11/24 interface=MGMT network=10.13.2.0\n# Global Settings\n/ip route add dst-address=0.0.0.0/0 gateway=10.13.2.1\n/ip dns\nset servers=10.13.2.200,10.13.2.201\n/system identity\nset name=CRS317\n/system ntp client\nset enabled=yes primary-ntp=216.239.35.8 secondary-ntp=216.239.35.4\n/user set 0 password=\"password\"\n# Set Jumbo frames\n/interface ethernet\nset sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus16 l2mtu=10218\nset sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus16 mtu=9000\n# Bridge Configuration\n/interface bridge\nadd name=bridge vlan-filtering=no mtu=9000\n/interface bridge port\nadd bridge=bridge interface=sfp-sfpplus1 hw=yes disabled=no\nadd bridge=bridge interface=sfp-sfpplus2 hw=yes disabled=no\nadd bridge=bridge interface=sfp-sfpplus3 hw=yes disabled=no\nadd bridge=bridge interface=sfp-sfpplus4 hw=yes disabled=no\nadd bridge=bridge interface=sfp-sfpplus16 hw=yes disabled=no\nset bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged [find interface=sfp-sfpplus1] \nset bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged [find interface=sfp-sfpplus2]\nset bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged [find interface=sfp-sfpplus3]\nset bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged [find interface=sfp-sfpplus4]\nset bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged [find interface=sfp-sfpplus16]\n/interface bridge vlan\nadd bridge=bridge tagged=sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus16 vlan-ids=2611\nadd bridge=bridge tagged=sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus16 vlan-ids=2612\nadd bridge=bridge tagged=sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus16 vlan-ids=2613\n/interface vlan\nadd name=2611-MGT interface=bridge vlan-id=2611 mtu=9000\nadd name=2612-vMotion interface=bridge vlan-id=2612 mtu=9000\nadd name=2613-VM interface=bridge vlan-id=2613 mtu=9000\n/interface bridge set bridge vlan-filtering=yes ingress-filtering=yes frame-types=admit-only-vlan-tagged\n\t\t\t\t\n\nMikroTik CCR2004-1G-12s+2XS Config\nA bit more tricky to configure is the MikroTik CCR2004 router. Since this will be routing between various VLANs and the rest of my home network, the configuration is a bit more nuanced. However, it is still really easy to understand once you get it up and running. Unlike the CRS317, I\u2019m NOT connecting the management port on the CCR2004, as the router has an uplink to the house LAN that I can use to configure the switch. The running configuration of my router is below. Again, if you reset the router to factory defaults and remove the default configuration, you can simply copy/paste the config below for the full configuration without touching the UI.\n\t\t\t\t\n\t\t\t\t\t# Global settings \n/ip route add dst-address=0.0.0.0/0 gateway=10.13.2.1 \n/ip dns \nset servers=10.13.2.200,10.13.2.201 \n/system identity \nset name=CCR2004 \n/system ntp client \nset enabled=yes primary-ntp=216.239.35.8 secondary-ntp=216.239.35.4 \n/user set 0 password=\"password\" \n# Set Jumbo frames \n/interface ethernet \nset sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus5,sfp-sfpplus6,sfp-sfpplus7,sfp-sfpplus8,sfp-sfpplus9,sfp-sfpplus10,sfp-sfpplus11,sfp-sfpplus12 l2mtu=9578\nset sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus4,sfp-sfpplus5,sfp-sfpplus6,sfp-sfpplus7,sfp-sfpplus8,sfp-sfpplus9,sfp-sfpplus10,sfp-sfpplus11,sfp-sfpplus12 mtu=9000\n# Setup Bridge \n/interface bridge \nadd name=bridge vlan-filtering=no mtu=9000\n/interface bridge port \n# upstream port will be untagged (access) port of VLAN ID 42 \nadd bridge=bridge pvid=42 ingress-filtering=yes frame-types=admit-only-untagged-and-priority-tagged interface=sfp-sfpplus1\n# The rest of SFP+ ports are bridged together, all tagged \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus2 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus3 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus4 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus5 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus6 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus7 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus8 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus9 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus10 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus11 disabled=yes \nadd bridge=bridge ingress-filtering=yes frame-types=admit-only-vlan-tagged interface=sfp-sfpplus12 \n/interface bridge vlan \n# sfp-sfpplus1 gets added as untagged automatically due to pvid setting above\nadd bridge=bridge tagged=bridge vlan-ids=42  \nadd bridge=bridge tagged=bridge,sfp-sfpplus12 vlan-ids=2611 \nadd bridge=bridge tagged=bridge,sfp-sfpplus12 vlan-ids=2612 \nadd bridge=bridge tagged=bridge,sfp-sfpplus12 vlan-ids=2613 \n/interface vlan \nadd name=WAN interface=bridge vlan-id=42 \nadd name=2611-MGT interface=bridge vlan-id=2611 mtu=9000\nadd name=2612-vMotion interface=bridge vlan-id=2612 mtu=9000\nadd name=2613-VM interface=bridge vlan-id=2613 mtu=9000\n/ip address \nadd address=10.13.2.10/24 interface=WAN \nadd interface=2611-MGT address=172.26.11.1/24 \nadd interface=2612-vMotion address=172.26.12.1/24 \nadd interface=2613-VM address=172.26.13.1/24 \n/interface bridge set bridge vlan-filtering=yes\n\t\t\t\t\n\nSummary\nSetting up a new lab is always exciting, and a great learning experience. Getting the MikroTik configurations exactly right did take some time and research. But RouterOS is very flexible, and there are great support forums for questions. So far I\u2019m happy with the gear I\u2019ve purchased, and it should lead to a lot more blog content over the coming years. If you want to setup a similar configuration, you can easily take the running configs I have and modify them as needed to suite your needs.\u00a0\n"}, {"name": "Proxmox GRID vGPU", "date": "30.12.2021", "author": "admin", "url": "https://blog.iservery.com/2021/12/30/proxmox-grid-vgpu/", "content": "\nhttps://veil-update.mashtab.org/drivers/nvidia_drivers/\n\nProxmox 7 vGPU\n\nhttps://theorangeone.net/posts/lxc-nvidia-gpu-passthrough/\n"}, {"name": "RESEARCH ON PERFORMANCE TUNING OF HDD-BASED CEPH* CLUSTER USING OPEN CAS", "date": "12.9.2021", "author": "admin", "url": "https://blog.iservery.com/2021/09/12/research-on-performance-tuning-of-hdd-based-ceph-cluster-using-open-cas/", "content": "\nsource is here https://01.org/blogs/tingjie/2020/research-performance-tuning-hdd-based-ceph-cluster-using-open-cas\nPREFACE\nCeph* is a widely used distributed-storage solution. The performance of Ceph varies greatly in different configuration environments. Many clusters in production environments are deployed on hard disks. For various types of workloads, performance requirements are also different.\nWe built a Ceph cluster based on the Open-CAS caching framework. We made some adjustments to the characteristics so the system could cope with workloads that support large-scale sequential access. The adjustments provided better support for trading-system applications based on random access with a small block size.\nSCENARIO\nThe random access of HDD is restricted by the seek time of the magnetic head, which causes the performance of random access to drop very severely compared with an SSD. For a 10,000 RPM mechanical hard disk, the IOPS (input/output operations per second) of random read and write is only about 350. Ceph clusters based on mechanical hard drives cost less and are suitable for sequential access loads of large-scale data but are not suitable for small-block data access in OLTP (On-line transaction processing) workload. How can we improve the access performance of small random operations with competitive costs? We propose the Open-CAS caching framework to accelerate Ceph OSD nodes.\nThe baseline and optimization solutions are shown in Figure 1 below.\n\nFigure 1: Ceph cluster performance optimization framework based on Open-CAS\nBaseline configuration: An HDD is used as a data partition of BlueStore, and metadata (RocksDB and WAL) are deployed on Intel\u00ae Optane\u2122 SSDs.Optimized configuration: An HDD and NVMe* SSD are combined into a new CAS device through Open-CAS software as the data partition of BlueStore, and metadata (RocksDB and WAL) are deployed on Intel Optane SSDs.\nOpen-CAS is an open-source cache acceleration solution initiated by Intel. By loading the modules into the kernel, the high-speed media device(s) used as cache, \u201cmerge\u201d with slow devices into a new CAS device, thereby improving the overall device read and write performance of the system.\nThe hierarchical structure of Open-CAS in the system is shown in Figure 2 below. OCF (core of Open-CAS, a high-performance block storage cache library written in C) is the filesystem layer showing as below. It is the IO request processing based on the block device layer.\n\nFigure 2: \u00a0Open-CAS framework\nThis article focuses on how to configure and optimize Open-CAS and build a Ceph cluster with it.\nPERFORMANCE EVALUATION\nSETUP\nWe built a distributed storage cluster based on Ceph. On the server side, the Ceph cluster is deployed by CeTune to provide storage services. On the client side, we deployed the Vdbench benchmark to verify our expectation. Vdbench is a popular IO benchmark tool written in Java*, which supports multi-process concurrent read and write tests on block devices in various modes.\nTable1: Ceph Cluster Configuration\nHardware Configuration\u00a0\u00a0\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CPUServer: 2 * Intel\u00ae Xeon\u00ae Platinum 8260L CPU @ 2.40GHzClient: Intel\u00ae Xeon\u00ae Gold 6252 CPU @ 2.10GHz\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0MemoryServer:\u00a03 * 192 GB\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Network SwitchBandwidth 100 Gb\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Intel Optane SSD2 * Intel P4800X 375 GB\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0NVMe SSD6 * Intel P4510 1 TB\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HDD12 * 1 TB 1W SAS HDDSystemConfiguration\u00a0\u00a0\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OSUbuntu* 18.04.5 LTS\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Kernel VersionLinux* version 5.4.0-48-generic\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ceph VersionNautilus 14.2.11\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Ceph Cluster2 Server node, 1 Client node, replica = 2\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Open-CAS Version20.09.0.0362.master\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Vdbench Verson5.04.07\u00a0\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0For each Ceph Node\u00a0\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Intel Optane SSD2 * 375 GB\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0NVMe SSD3 * 1.0 TB (6 * 250G)\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HDD6 * 1.0 TB\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OSD Number6 (HDD:OSD = 1:1)\nOPTIMIZED CONFIGURATION OF OPEN-CAS\nFor the optimized solution, we used Open-CAS for the data partition. Open-CAS can be used after compilation and installation:\u00a0https://open-cas.github.io/guide_introduction.html.\nThe optimized configuration in Open-CAS is shown below:\n# casadm -S -d /dev/nvme2n1p1 -c wb   --force       // Create a new cache device, and return cache ID\n# casadm -A -i -d /dev/sda                          // Add the backend device to the cache device and \u201cmerge\u201d into a new CAS device\n# casadm -L                                         // View current Open-CAS configuration information\ntype    id   disk             status    write policy   device\ncache   1    /dev/nvme2n1p1   Running   wb             -\n\u2514core   1    /dev/sda         Active    -              /dev/cas1-1\ncache   2    /dev/nvme2n1p2   Running   wb             -\n\u2514core   1    /dev/sdb         Active    -              /dev/cas2-1\ncache   3    /dev/nvme3n1p1   Running   wb             -\n\u2514core   1    /dev/sdc         Active    -              /dev/cas3-1\ncache   4    /dev/nvme3n1p2   Running   wb             -\n\u2514core   1    /dev/sdd         Active    -              /dev/cas4-1\ncache   5    /dev/nvme4n1p1   Running   wb             -\n\u2514core   1    /dev/sde         Active    -              /dev/cas5-1\ncache   6    /dev/nvme4n1p2   Running   wb             -\n\u2514core   1    /dev/sdf         Active    -              /dev/cas6-1\n \n// The configuration for each cache ID\n# casadm -X -n seq-cutoff -i 1 -j 1 -p always -t 16   // seq-cutoff always and threshold 16KB\n \n# casadm --io-class --load-config --cache-id 1 -f ioclass-config.csv // only cache with request_size <=128K\n# cat ioclass-config.csv\nIO class id,IO class name,Eviction priority,Allocation\n0,unclassified,22,0\n1,request_size:le:131072,1,1\n \n# casadm -X -n cleaning-alru -t 1000 -i 1              // clean policy alru with activity threashold to 1s\n\u00a0\nWe made several optimizations for the above configuration:\nSequential read and write cut-off: When the IO stream of sequential read and write reaches a certain threshold, the cut-off is turned on. All subsequent read and write requests go directly to backend storage until random access causes the cut-off to be terminated.IO classification and priority definition: We hope to classify the random access by request size, and only cache the data with blocks less than 128K. This requires adding a category based on request_size and setting the eviction priority to high while avoiding too easy to be evicted from the cache.Parameter adjustment of clean policy: Set the default alru policy and shorten the background cleaning reaction time. There are two types of cleaning policies: alru and acp. acp is a more active strategy, but alru is more suitable for when there is free space in the cache, it does not consume too much bandwidth for background cleaning.\nCEPH CLUSTER SETUP AND CONFIGURATION\nWe deployed a three-node Kubernetes* cluster through the CeTune tool; two nodes are for Ceph storage nodes, and one node is for the client.\nCeTune is a framework for deployment, benchmarking, and configuration and adjustment of Ceph cluster performance. It integrates some benchmarking tools and provides various parameter data for system indicators.\nYou can refer to the official documentation to compile and install. Before the deployment, we need to take care multiple configuration items:\nStorage node configuration OSD according to the following format: osd:data:db_wal. Each OSD requires three disks, corresponding to the information of the OSD, the data partition of OSD, and metadata partition of OSD.Network configuration. There is a public network, a cluster network, and a separated Ceph monitor network.\nThe configuration file is in conf/all.conf; the main contents are shown below:\nhead=CephCAS1                               # Head node\nlist_server=CephCAS1,CephCAS2               # OSD nodes\nlist_client=CEPHCLIENT-01\nlist_mon=CephCAS1\ndisk_format=osd:data:db_wal\nCephCAS1=/dev/cas1-1p1:/dev/cas1-1p2:/dev/nvme1n1p5,/dev/cas2-1p1:/dev/cas2-1p2:/dev/nvme1n1p6,/dev/cas3-1p1:/dev/cas3-1p2:/dev/nvme1n1p7,/dev/cas4-1p1:/dev/cas4-1p2:/dev/nvme1n1p8,/dev/cas5-1p1:/dev/cas5-1p2:/dev/nvme1n1p9,/dev/cas6-1p1:/dev/cas6-1p2:/dev/nvme1n1p10\nCephCAS2=/dev/cas1-1p1:/dev/cas1-1p2:/dev/nvme1n1p5,/dev/cas2-1p1:/dev/cas2-1p2:/dev/nvme1n1p6,/dev/cas3-1p1:/dev/cas3-1p2:/dev/nvme1n1p7,/dev/cas4-1p1:/dev/cas4-1p2:/dev/nvme1n1p8,/dev/cas5-1p1:/dev/cas5-1p2:/dev/nvme1n1p9,/dev/cas6-1p1:/dev/cas6-1p2:/dev/nvme1n1p10\n\u2026\npublic_network=192.168.10.0/24             # Based on 100Gb NIC\nmonitor_network=192.168.10.0/24\ncluster_network=192.168.11.0/24\n\u00a0\nThen we executed the deployment script and created a storage pool.\n# python run_deploy.py redeploy --gen_cephconf                        // Deploy Ceph cluster and wait for finish\n\u2026\n# ceph osd pool create rbd 512 512                                    // 12 OSDs, Set PG num & PGP num to 512 is appropriate\n# ceph osd pool set rbd size 2                                        // Set pool replicated = 2\n# ceph osd pool application enable rbd  rbd --yes-i-really-mean-it\n \n# rbd create --thick-provision --size 65536 rbd/rbd1 --image-format 2 // Create rbd and fill in content atumaticaly, it will take more time\n\u2026\n# rbd map rbd/rbd1                                                    // Map rbd block device on the client side\n\u00a0\nPERFORMANCE AND COMPARATIVE ANALYSIS\nVDBENCH BENCHMARK ANALYSIS\nTEST ENVIRONMENT AND REPORT\nThe test configuration of Vdbench on the Ceph RBD block device is listed below.\nTest environment\u00a0\u00a0\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0DUT number1\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Baseline Environment12 * 1 TB HDD as OSD\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Optimization Environment12 * 1 TB HDD and 12 * 250 GB NVMe SSD combination devices as OSD\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0RBD volume size20 * 64 GB\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Read and write mode4K Random Read4K Random Write4K Random Read(70%) and Random Write(30%)512K Sequential Read512K Sequential Write\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Queue depth16\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Threads20\u00a0\u2013\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Duration600 seconds\nThe test report is shown in the graphs below.\n\nFigure 3: Raw performance results for random I/O\n\n\u00a0Figure 4: Raw performance results for sequential I/O\n\nFigure 5: Open-CAS configuration comparison\nAs you can see from the figure above, in the Open-CAS optimization configuration, the IOPS of random reads (100% cache hits) and random writes has increased by 119.54 times and 86.761 times. The average latency has been reduced to 0.8% and 1.2%, respectively.\nFor sequential reads and writes, we pass the large block of sequential writes directly to the backend and evaluate the performance loss of sequential read and write at the CAS layer. The bandwidth loss of sequential read is 2.8%, and for sequential writes is 5.6%.\nCONFIGURATION STRATEGY OF OPEN-CAS\nThe performance improvement of random access mainly comes from the characteristics of random access of NVMe SSDs. Open-CAS is the equivalent of glue, which combines the advantage of random access of NVMe SSDs with the advantage of HDDs large capacity.\nFor sequential access, multiple HDDs can be used to increase parallelism in a production environment.\u00a0 The more HDDs in a node, the better the performance of sequential access. This parallelism can offset the impact of limited performance of a single HDD.\n\nFigure 6: R/W performance comparison\n\u00a0SAS HDD RandWriteSAS HDD RandReadNVMe SSD RandWrite\u00a0(Intel P4510)NVMe SSD RandRead\u00a0(Intel P4510)Bandwidth (MB/s)1.311.17983947Average Latency (us)95640.00107043.07125.32130.99\nThe mechanism used by Open-CAS to accelerate the write process mainly depends on whether there is free or clean cache space in the cache pool. When the cache pool is completely \u201ccontaminated\u201d by dirty data, the cache pool is invalid for writes, which is also a common characteristic of the cache system.\nThe figure below shows the percentage of \u201cdirty\u201d cache during random writes. As the time goes by, more and more data are written to cache. When the buffer is \u201cdirty\u201d at all, for data consistency consideration, it must be flushed back to backend storage to release the cache space.\n\nFigure 7: Open-CAS dirty rate % increase\nThere are two solutions: increasing \u201cincome\u201d and reducing \u201cexpenditure\u201d.\nIncreasing \u201cincome\u201d means to improve the cache flushing policy. If you flush the cache data to the backend storage faster, then the \u201ccontaminated\u201d cache space can be released sooner. The improvement is not obvious since the bottleneck is the low performance of random write to HDD when flushing.Reducing \u201cexpenditure\u201d can make for optimal use of cache resources. Several configurations, such as cache-line settings, sequential access cut-off, and IO classification can be accounted into here. The\u00a0configuration: seq-cutoff\u00a0bypasses the cache and directly writes to backend storage when sequential IO stream is detected. By this way, only random accessed data will be cached.\nThe performance improvement of random read mainly depends on the read hit rate (read_hit_rate = read_cache_hit_number / total_read_access_number). We verified that 100% hit rate is the ceiling for performance improvement. In the actual environment, the read hit rate depends on many factors, such as the cache capacity and the data access mode. The read hit rate improvement requires a comprehensive design from the application to the cache layer.\nThe following are read and write conditions in the Ceph OSD device (Open-CAS device combined with cache device and backend device) with different access modes, which is collected by the dstat tool:\nDue to the setting of sequential cut-off, the sequential access of large block will store to the backend storage directly.\n\nFigure 8: Sequential read on OSD\n\nFigure 9: Sequential write on OSD\nFor random writes, if the write hits or there is a free or clean cache block, the data can be cached. The following figure shows the ideal situation for all writes to be cached.\n\nFigure 10: Random write on OSD\nIn the case of random read (100% hit rate), all data is read from the cache and uses the full cache capability.\n\nFigure 11: Random read on OSD\nCONCLUSIONS AND RECOMMENDATIONS\nThe configuration described in this article is applicable to general scenarios. It is applicable to the workload with large-scale sequential access and concentrated random access to the Ceph cluster.\nFor the sequential access pattern, the performance advantage of SSD over HDD is not obvious. We set Open-CAS to sequential access cut-off and to directly access backend storage, which can save valuable cache pool resources, and maximizes its capacity for small random-access data. Of course, some scenarios require short-term high performance and low latency. For these scenarios, try full read and write caching. The specific configuration depends on the number and concurrency of HDD and SSD (cache) devices.\nFor random write access with a small block size, the performance depends on the write hit rate and the capacity of clean or free cache blocks in the cache pool. Under ideal conditions (such as clean and free cache blocks or a very high write hit rate), in write-back mode, the write request directly returns from the cache to the application. When the cache pool is full of dirty cache blocks and the write hit rate is very low, the write performance will drop sharply because new write access data needs to be promoted to cache blocks and must wait for the old dirty blocks to be flushed to the backend storage.\nFor random read access with a small block size, the performance depends on the read hit rate. In case of a read miss, the cache needs to access the backend storage to fetch the data and promote it to the cache. In extreme cases, additional flushing of data to backend storage is required.\nHere are several ways to improve the read hit rate on the cache side:\nUse different cache modes, such as write around mode, write invalid mode, etc.The promotion strategy can be set according to the application access patternUse application pre-heating dataOptimize the application access data model\nFor general scenarios, Open-CAS reference configuration is write-back mode, cache-line is 4KB, sequential access cut-off is always on, set IO classification and only cache small random block (request size <=128KB), and use clean policy with default alru while more active parameters adjustment. In an ideal situation, random read IOPS is increased by 119.54 times, random write IOPS is increased by 86.761 times, and access latency is reduced to 0.8% and 1.2% respectively.\nSUMMARY AND OUTLOOK\nIn the HDD-based storage environment, the Ceph cluster with Open-CAS on NVMe SSD as a storage node, cache has significantly improved the performance of the Ceph client block storage for small block random read and write. The replication mechanism in the Ceph storage node ensures the reliability of cached data and write-back mode is suitable for a Ceph storage cluster.\nOpen-CAS supports multiple caching modes and has corresponding preferred configurations for read-only, write-only, and read-write modes. Using the Open-CAS caching framework can take advantage of the high speed random read and write of NVMe SSDs and the large capacity of HDDs.\nThere are several business scenarios that have reference value:\nRandom read and write scenarios of small data blocks with low latency requirements, such as online transaction systems and banking services, which can take advantage of the high-speed random read and write capabilities of NVMe SSD.Large throughput scenarios with massive data, such as video on demand, big data analysis and processing, etc. The multiple concurrent HDD pass-through mode can ensure stable bandwidth and sequential read and write capabilities.\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\nREFERENCE\nOpen-CAS:\u00a0https://open-cas.github.io/CeTune and github\uff1ahttps://github.com/intel/CeTune\nAPPENDIX\nOPEN-CAS PARAMETERS DESCRIPTION\nThere are many parameters of Open\u00a0CAS; Based on our experience and the testing environment, we evaluated and verified several key parameters that effect performance.\nCACHE MODE\nThere are multiple cache modes supported by Open-CAS. We expect to figure out the mode on mixed read/write operations and majorly to improve random access with small block size. Eventually, we choose write-back mode.\nOn the client side, only kept one copy on the local disk. If the disk is physically damaged, the data will be lost permanently. We deploy the caching solution on the Ceph storage cluster with replication guaranteed which avoids the single point failure.\nThe read/write flow of the write-back mode is shown in the figure below. In addition to the read-write process, Open-CAS flushes dirty data (the cached data is inconsistent with backend storage data) to backend storage according to the cleaning policy.\n\nFigure 12: IO request flow\nCACHE-LINE SIZE\nA cache line is the smallest portion of data that can be mapped into a cache. Every mapped cache line is associated with a core line, which is a corresponding region on a backend storage. The relationship between a cache line and a core line is illustrated on the figure below.\n\nFigure 13: Cache mappings\nThe default size of the Open-CAS cache line ranges from 4K up to 64K, as supported by the current system. The setting is specified when the cache device is created and cannot be modified at runtime.\nFor cache devices with NVMe SSDs, there is no seek time impact of using HDD. There is not much difference in bandwidth for reading small blocks versus large blocks. The larger the cache line setting, the larger the cache space occupied by read/write requests; for access with a small block size, it is also a waste of cache space. In some high throughput scenarios, such as with Intel\u00ae Optane\u2122 Persistent Memory, you may need to increase the cache line to increase bandwidth.\nSEQUENTIAL ACCESS CUT-OFF\nWhen the sequential IO stream reaches a certain threshold, the cut-off is turned on. All subsequent sequential read and write requests are sent directly to the backend storage, until cut-off is terminated.\nFor small random blocks, the IOPS of SSD has obvious advantages over HDD. However, for large block sequential read and write, the advantage of SSD as a cache is not obvious, especially as one SSD acts as a cache for multiple HDDs. Therefore, many cache solutions choose to bypass read/write of sequential large blocks and send them directly to backend HDD devices. It can save cache space and accommodate for more data of random access with small block size.\nWe evaluated the performance of SAS HDD disks in different modes and block sizes by using the FIO (jobs=4, queue_depth=8) benchmark. We found that the random read and write performance bandwidth increased with the block size (IOPS did not change much), and sequential read and write performance is less affected by block size.\n\nFigure 14: SAS HDD performance with FIO\nCLEAN POLICY\nIn the cache system, Open-CAS provides the flushing strategies and according parameters to have data flushed from cache to backend storage.\nThe default policy,\u00a0alru, periodically refreshes dirty data. Using a modified least-recently-used algorithm, the refresh priority is relatively low. Another cleaning policy,\u00a0acp, clears dirty cache lines as quickly as possible to maintain the maximum bandwidth of backend storage. The acp policy aims to stabilize the data refresh time of write-back mode and maintain more consistent cache performance.\nWe found that the effect of the acp policy is not satisfactory. When performing normal IO operations in the foreground, the acp mode is more active and the flush continues indefinitely. As a result, the performance of read and write operations in the foreground is seriously affected. We set the refresh policy to alru and adjusted the parameters to make the flush operation more active. When the system has free or clean cache space, the alru policy is better.\n\nOTHERS\nOpen-CAS also supports many-to-one mode, in which multiple backend storage devices can be cached by a cache device and the data of these backend storage devices share a cache pool. The advantage is the data access can be balanced. In Ceph, the data distribution of each OSD is not necessarily completely balanced. Sharing a cache pool offsets the cache space waste caused by data imbalance (a typical OSD data distribution is shown below). But the disadvantage in the write-back cache mode is that a single point of failure is unavoided. If multiple OSDs share data in a storage pool, and the storage pool contains all copies of this data, the situation becomes complicated.\nTherefore, we use a one-to-one model, where one cache device corresponds to one backend storage device.\n\nIn addition to these parameters, Open-CAS also provides some media related configurations, such as the support of trim and atomic write, but this requires a specific cache medium and kernel version\n"}, {"name": "N\u00e1hrada google analytics \u2026", "date": "8.9.2021", "author": "admin", "url": "https://blog.iservery.com/2021/09/08/nahrada-google-analytics/", "content": "\nhttps://github.com/PostHog/posthog-foss\n"}, {"name": "kubernetes ceph atd", "date": "8.9.2021", "author": "admin", "url": "https://blog.iservery.com/2021/09/08/kubernetes-ceph-atd/", "content": "\n\nzdroj https://cloud.netapp.com/blog/kubernetes-shared-storage-the-basics-and-a-quick-tutorial\nIntroduction\nKubernetes, the awesome container orchestration tool is changing the way applications are being developed and deployed. You can specify the required resources you want and have it available without worrying about the underlying infrastructure. Kubernetes is way ahead in terms of high availability, scaling, managing your application, but storage section in the k8s is still evolving. Many storage supports are getting added and are production-ready.\nPeople are preferring clustered applications to store the data. But, what about the non-clustered applications? Where do these applications store data to make it highly available? Considering these questions, let\u2019s go through the Ceph storage and its integration with Kubernetes.\nWhat is Ceph Storage?\nCeph is an open source, software-defined storage maintained by RedHat. It\u2019s capable of the block, object, and file storage. The clusters of Ceph are designed in order to run on any hardware with the help of an algorithm called CRUSH (Controlled Replication Under Scalable Hashing). This algorithm ensures that all the data is properly distributed across the cluster and data quickly without any constraints. Replication, Thin provisioning, Snapshots are the key features of the Ceph storage.\nThere are good storage solutions like Gluster, Swift but we are going with Ceph for following reasons:\nFile, Block, and Object storage in the same wrapper.Better transfer speed and lower latencyEasily accessible storage that can quickly scale up or down\nWe are going to use 2 types of storage in this blog to integrate with Kubernetes.\nCeph-RBDCephFS\nCeph Deployment\nDeploying highly available Ceph cluster is pretty straightforward and easy. I am assuming that you are familiar with setting up the Ceph cluster. If not then refer the official document\u00a0here.\nIf you check the status, you should see something like:https://medium.com/media/6ac14a1f7cd854b828c34b4b9e227a43\nHere notice that my Ceph monitors IPs are 10.0.1.118, 10.0.1.227 and 10.0.1.172\nK8s Integration\nAfter setting up the Ceph cluster, we would consume it with Kubernetes. I am assuming that your Kubernetes cluster is up and running. We will be using Ceph-RBD and CephFS as storage in Kubernetes.\nCeph-RBD and Kubernetes\nWe need a\u00a0Ceph RBD\u00a0client to achieve interaction between Kubernetes cluster and CephFS. This client is not in the official kube-controller-manager container so let\u2019s try to create the external storage plugin for Ceph.\nCheck the repo\u00a0here\nhttps://medium.com/media/b65d12f56f2f591ec6262f08c7fbbd6chttps://medium.com/media/d84e860563ee377027d5c59333765491\nYou will get output like this:\n\nhttps://medium.com/media/6953df5fa5e1e81a142e66c2933fbdb1\n\nCheck RBD volume provisioner status and wait till it comes up in running state. You would see something like the following:\n\nhttps://medium.com/media/63d38ab2e4fe984456270e600edd0420\n\nOnce the provisioner is up, provisioner needs the admin key for the storage provision. You can run the following command to get the admin key:\n\nhttps://medium.com/media/db5d9f84eea1b9114cefa04fa80ca6e8\n\nLet\u2019s create a separate Ceph pool for Kubernetes and the new client key:\n\nhttps://medium.com/media/f89bf6b39192538047a7f64f939e5421\n\nGet the auth token which we created in the above command and create kubernetes secret for new client secret for kube pool.\n\nhttps://medium.com/media/af9cbb297aa99f11cf0da0f4dbae4a70\n\nNow let\u2019s create the storage class.\nhttps://medium.com/media/9103199e02f9bf49943f6b891efeb29fhttps://medium.com/media/414761ead76d04c9423b001c88b4069e\nWe are all set now. We can test the Ceph-RBD by creating the PVC. After creating the PVC, PV will get created automatically. Let\u2019s create the PVC now:\nhttps://medium.com/media/0ac38aea05fe17e083cedf2502d3f52ehttps://medium.com/media/35914a1638c566ec559834b9da6e15ed\nIf you check pvc, you\u2019ll find it shows that it\u2019s been bounded with the pv which got created by storage class.Let\u2019s check the persistent volume\n\nhttps://medium.com/media/96f819b98475214f079cfda96eb09e9e\n\nTill now we have seen how to use the block based storage i.e Ceph-RBD with kubernetes by creating the dynamic storage provisioner. Now let\u2019s go through the process for setting up the storage using file system based storage i.e. CephFS.\nCephFS and Kubernetes\nLet\u2019s create the provisioner and storage class for the CephFS. Create the dedicated namespace for CephFS\n\nhttps://medium.com/media/70dc047b8c979f72cdd685ec7871a763\n\nCreate the kubernetes secrete using the Ceph admin auth token\n\nhttps://medium.com/media/6a8259202a9db55dea8253c624aacf2a\n\nCreate the cluster role, role binding, provisioner\nhttps://medium.com/media/e92bfc6149169126fb84d927d566cf25https://medium.com/media/937a95e69c08dd6dbbd47c957dc9f093\nCreate the storage class\nhttps://medium.com/media/9770f5d3f54c6892e1005fa5898f80f0https://medium.com/media/6726614b898ddd6a5aa436882f160fc1\nWe are all set now. CephFS provisioner is created. Let\u2019s wait till it gets into running state.\n\nhttps://medium.com/media/b23e7a398b50a495e82a9c321cbf90bc\n\nOnce the CephFS provider is up, try creating the persistent volume claim. In this step, storage class will take care of creating the persistent volume dynamically.\nhttps://medium.com/media/f0f2fc0967f95fb6793ee78045b6a668https://medium.com/media/d2e0230ded5199d679363ba316331cf9\nLet\u2019s check the create PV and PVC\n\nhttps://medium.com/media/b5b206170860f1a508d1627f2cef52c9\n\nConclusion\nWe have seen how to integrate the Ceph storage with Kubernetes. In the integration, we covered\u00a0ceph-rbd\u00a0and\u00a0cephfs. This approach is highly useful when your application is not a clustered application and if you are looking to make it highly available.\n*****************************************************************\nThis post was originally published on\u00a0Velotio Blog.\nVelotio Technologies\u00a0is an outsourced software product development partner for technology startups and enterprises. We specialize in enterprise B2B and SaaS product development with a focus on artificial intelligence and machine learning, DevOps, and test engineering.\nInterested in learning more about us? We would love to connect with you on ourWebsite,\u00a0LinkedIn\u00a0or\u00a0Twitter.\n"}, {"name": "Spannig tree protocol", "date": "12.4.2021", "author": "admin", "url": "https://blog.iservery.com/2021/04/12/spannig-tree-protocol/", "content": "\n\n\n\n\nhttps://www.ciscopress.com/articles/article.asp?p=2832407&seqNum=6\n\n\nPlease enable JavaScriptplay-sharp-fill\n\n\n\nLinkEmbedCopy and paste this HTML code into your webpage to embed.\n\n\nSTP Overview\nSpanning Tree Protocol\nTypes of STP\u2013 Original STP\u2013 STP / 802.1D\u2013 PVST+\u2013 Cisco improvement adding a per VLAN feature\u2013 Cisco default\u2013 RSTP / 802.1w\u2013 Improved STP with much faster convergence\u2013 Rapid PVST+\u2013 Cisco improvement of RSTP adding per VLAN feature\u2013 Makes a large network more efficient\nWhy STP?\u2013 STP is used to prevent loops when using redundant switches\u2013 Broadcast messages are sent all the time and Broadcast Storms are easy to trigger\u2013 Loops also cause unstable MAC address tables because they\u2019re constantly being changed/updated\u2013 Duplicate frames are being sent to the same host\nHow STP Works\u2013 Switches in a loop \u201adrop\u2018 one of the ports\u2013 Switch with the blocked port still receives the data but it ignores it\u2013 Simple but how the switches choose the port to block can be tricky\nChoosing the blocked port\u2013 1) Elect a root bridge\u2013 King of switches\u2013 2) Place root bridge interfaces into a Forwarding state\u2013 3) Each non-root switch selects its Root Port\u2013 This is the best route to the bridge\u2013 4) Remaining links choose a Designated Port\u2013 5) All other ports are put into a Blocking state\nRoles\u2013 Root Ports\u2013 The best port to reach the Root Bridge\u2013 Designated Port\u2013 Port with the best route to the Root Bridge on a link\u2013 Non-Designated Ports\u2013 All other ports that are in a blocking state\nStates\u2013 Disabled\u2013 Port that is shutdown\u2013 Blocking\u2013 A port that is blocking traffic\u2013 Must move to Listening state before moving to Forwarding\u2013 Listening\u2013 Not forwarding traffic and not learning MAC addresses\u2013 Transitional State while changing from 1 role to another\u2013 Held in this state for the Forward Delay timer (15 sec default)\u2013 Learning\u2013 Not forwarding traffic but learning MAC addresses\u2013 Transitional State while changing from 1 role to another\u2013 Held in this state for the Forward Delay timer (15 sec default)\u2013 After this, the port can now move to a Forwarding state\u2013 Forwarding\u2013 Sending and receiving traffic like normal\u2013 Can move directly to Blocking\nRoot Bridge Election\u2013 Each switch has a BPDU. BPDU contains:\u2013 Root Cost\u2013 Cost of the root bridge\u2013 BID (Bridge ID)\u2013 The switch with the lowest overall BID will become the root bridge\u2013 They look something like: 32769aaaa:aaaa:aaaa\u2013 BID is made up of:\u2013 STP priority\u2013 Default value of 32768 + VLAN number\u2013 For VLAN 1, the STP priority would be 32769\u2013 MAC Address\u2013 Each switch thinks it should be the root bridge\u2013 They share their BPDUs with each other\u2013 Once they all agree, the root bridge has been elected\u2013 All ports on the root bridge enter a Forwarding State\u2013 Each non-root switch will now choose the best path to the root bridge\u2013 This is the Root Port\u2013 This is based on Port Cost\u2013 Cost is based on port speed\u2013 Better speed, lower cost\u2013 Each outgoing port to the root added together\u2013 Can be set manually\u2013 If a tie happens, they look at lowest neighbor BID\u2013 If they tie, they use lowest neighbor port priority\u2013 If they tie, lowest neighbor port number\u2013 Select a Designated Port\u2013 Look at lowest root cost to the bridge\u2013 If that ties, lowest BID\u2013 If that ties, lowest neighbor port priority\u2013 If that ties, lowest neighbor port number\u2013 Every port that is not a root port of designated port is put in a Blocking State\nDownfall of STP \u2014 Convergence\u2013 The time it takes to do the work and become stable\nTimers \u2013 Default (RSTP addresses the delay of convergence)\u2013 Hello\u2013 Every 2 seconds\u2013 Lets everyone know everything is still alive\u2013 MaxAge\u2013 10x Hello timer by default (20 seconds)\u2013 The time the switch will wait before it realizes something is wrong\u2013 Forward Delay\u2013 15 seconds\u2013 The time between the Listening and Learning state\n"}, {"name": "Dom\u00e1c\u00ed pancetta", "date": "12.4.2021", "author": "admin", "url": "https://blog.iservery.com/2021/04/12/domaci-pancetta/", "content": "\nhttps://www.prozeny.cz/clanek/domaci-pancetta-skvela-slanina-bez-uzeni-70854\n\n\nna 2 kusy\nNa maso\n1 b\u016f\u010dek bez kosticukr krupices\u016fl\nNa \u010desnekovo-bylinkovou variantu\n3\u00a0l\u017ei\u010dky provens\u00e1lsk\u00e9ho ko\u0159en\u00ed2\u00a0l\u017ei\u010dky su\u0161en\u00e9ho \u010desneku\nNa pikantn\u00ed variantu\n1\u00a0l\u017ei\u010dka cel\u00e9ho \u010dern\u00e9ho pep\u0159e5\u00a0kuli\u010dek jalovce2\u00a0l\u017e\u00edce mlet\u00e9 p\u00e1liv\u00e9 papriky\n\u010cl\u00e1nek\nPostup\nKrok 1:\u00a0Z\u00a0b\u016f\u010dku bez kosti\u00a0o\u0159\u00edzn\u011bte k\u016f\u017ei a p\u0159\u00edpadn\u00e9 b\u00edl\u00e9 blanky z povrchu. Po o\u010di\u0161t\u011bn\u00ed v\u00e1\u017eilo na\u0161e maso 600 gram\u016f. Roz\u0159\u00edzla jsem ho na poloviny pro dva typy ochucen\u00ed. Klidn\u011b ho nechte v celku \u2013 jen podle velikosti masa upravte d\u00e9lku su\u0161en\u00ed.Dom\u00e1c\u00ed su\u0161en\u00e1 panenka. Bez \u00e9\u010dek a\u00a0dusitan\u016f!Paprikovo-\u010desnekov\u00fd vep\u0159ov\u00fd b\u016f\u010dek\nKrok 2:\u00a0Maso omyjte, po\u0159\u00e1dn\u011b osu\u0161te a v uzav\u00edrateln\u00e9 misce ho zasypejte cukrem (cca 200 g), aby bylo cel\u00e9 zakryt\u00e9. Zav\u0159ete a nechte 2 dny (cca 36 a\u017e 48 hodin) v lednici. Pot\u00e9 maso vyndejte z cukru, d\u016fkladn\u011b omyjte a osu\u0161te a stejn\u00fdm zp\u016fsobem\u00a0nalo\u017ete do soli, tentokr\u00e1t jen na asi 30 hodin. Cukr a s\u016fl z masa vyt\u00e1hnou vodu a zakonzervuj\u00ed ho, nicm\u00e9n\u011b mu tak\u00e9 dodaj\u00ed chu\u0165, a \u010d\u00edm d\u00e9le v nich bude, t\u00edm bude nasl\u00e1dlej\u0161\u00ed, nebo naopak slan\u011bj\u0161\u00ed.\nKrok 3:\u00a0Ule\u017eel\u00e9 maso po\u0159\u00e1dn\u011b omyjte od soli, osu\u0161te a m\u016f\u017eete se pustit do obalov\u00e1n\u00ed v ko\u0159en\u00ed. Na jednu polovinu masa jsem pou\u017eila \u010derstv\u011b nadrcen\u00fd pep\u0159 a\u00a0jalovec\u00a0sm\u00edchan\u00fd s p\u00e1livou paprikou (pro extra odoln\u00e9 m\u016f\u017eete p\u0159idat chilli, pro jemn\u011bj\u0161\u00ed jaz\u00fd\u010dky naopak d\u00e1t jen sladkou papriku). Na druhou p\u016flku p\u0159i\u0161ly\u00a0provens\u00e1lsk\u00e9 bylinky\u00a0se su\u0161en\u00fdm \u010desnekem. S ko\u0159en\u00edm experimentujte, skv\u011bl\u00e1 je i kombinace\u00a0uzen\u00e9 papriky\u00a0a drcen\u00e9ho km\u00ednu nebo rozmar\u00fdnu s pep\u0159em a \u0161petkou mu\u0161k\u00e1tu. V\u016fbec nic tak\u00e9 nezkaz\u00edte jen samotn\u00fdm pep\u0159em.Nakl\u00e1dan\u00fd b\u016f\u010dek za studena: dobrota na ka\u017edou partyUzen\u00e9 maso va\u0159en\u00e9 v\u00a0kofole\nKrok 4:\u00a0Maso nap\u00edchn\u011bte na h\u00e1\u010dek, kus \u010dist\u00e9ho dr\u00e1tu, starou pletac\u00ed jehlici nebo jinou pom\u016fcku a pov\u011bste na v\u011btran\u00e9 m\u00edsto. Chladn\u011bj\u0161\u00ed m\u00edstnost je lep\u0161\u00ed, ale pokojov\u00e1 teplota nic nezkaz\u00ed. D\u016fle\u017eit\u00e9 je, aby okolo masa trochu proudil vzduch (tedy t\u0159eba na garn\u00fd\u017ei, u okna otev\u0159en\u00e9ho na mikroventilaci, v dom\u00e1c\u00ed vinot\u00e9ce a podobn\u011b).\nKrok 5:\u00a0Maso jsem nechala viset 16 dn\u016f a bylo u\u017e kr\u00e1sn\u011b prosu\u0161en\u00e9. M\u016f\u017eete za\u010d\u00edt ochutn\u00e1vat klidn\u011b u\u017e d\u0159\u00edv, nebo b\u016f\u010dek naopak dosu\u0161it \u00fapln\u011b dotuha. Doba su\u0161en\u00ed hodn\u011b z\u00e1le\u017e\u00ed na velikosti masa. Hotovou pancettu skladujte bu\u010f d\u00e1le pov\u011b\u0161enou, nebo v lednici.\u0160\u00e9fkucha\u0159i va\u0159\u00ed: Speciality z\u00a0masaMarinovan\u00fd losos na chleb\u00ed\u010dky \u2013 gravlax ze 4 surovin\nTakto su\u0161en\u00e9 maso je skv\u011bl\u00e9 jen tak na pl\u00e1tky k pivu a v\u00ednu, b\u00e1je\u010dn\u011b se hod\u00ed do\u00a0\u0161paget carbonara. Vynikaj\u00edc\u00ed je i\u00a0slan\u00fd cibulov\u00fd kol\u00e1\u010d s pancettou. Anebo zkus\u00edte\u00a0karban\u00e1tek balen\u00fd v pancett\u011b?\n"}, {"name": "Dell perc 6i linux", "date": "20.2.2021", "author": "admin", "url": "https://blog.iservery.com/2021/02/20/dell-perc-6i-linux/", "content": "\nhttps://null.53bits.co.uk/index.php?page=dell-omsa-on-linux\nDell OMSA on Linux\nWhere are the Dell repo\u2019s:\n\nhttp://linux.dell.com/wiki/index.php/Repository\n\nwget -q -O - http://linux.dell.com/repo/hardware/latest/bootstrap.cgi | bash\nyum install srvadmin-all\nOMSA changes for non-RHEL (CentOS) systems:\n/etc/redhat-release : \"Red Hat Enterprise Linux Server release 5 (Tikanga)\"\nOMSA Live CDs:\n\nhttp://linux.dell.com/files/openmanage-contributions/\n\nCentOS Live CDs:\nhttp://wiki.centos.org/Manuals/ReleaseNotes\u00a0(At the bottom you fool)\nPre reqs for BIOS/Firmware updates:\nyum install compat-libstdc*  libstdc++\nOMSA on Ubuntu;\nReferences;http://www.keithscode.com/linux-tutorials/installing-dell-openmanage-server-administrator-on-ubuntu.htmlhttp://ubuntuforums.org/showpost.php?p=2336772&postcount=54\nhttp://www.ccn.ucla.edu/users/jkyle/weblog/e14bd/Dell_Perc_5i_on_Ubuntu_64.html\n\n# Kernel modulessudo modprobe ipmi_msghandlersudo modprobe ipmi_devintfsudo modprobe ipmi_s# Sometimes this is `sudo modprobe ipmi_si` on 64bit machines\n\n\necho \"ipmi_msghandler\" >> /etc/modulesecho \"ipmi_devintf\" >> /etc/modulesecho \"ipmi_si\" >> /etc/modules\n\n\n# To use a repo stop here and perform add to /etc/apt/sources.list# \"deb ftp://ftp.sara.nl/pub/sara-omsa dell sara\"# then run;# wget http://ftp.sara.nl/debian_sara.asc# sudo apt-key add debian_sara.asc# sudo apt-get update # Now there is no need to download the debian packages below\n\n\n#Install IPMI tools;#yum install openipmi ipmitoolsudo apt-get install openipmisudo apt-get install ipmitool #Install OMSA dependancies:sudo apt-get install snmp snmpdsudo apt-get install -f lib32ncurses5 #sudo apt-get install -f libncurses5 sudo apt-get install -f ia32-libs #Only needed for 64bit machines# on 64bit machines; # dpkg --add-architecture i386 # this will enable the installation of i386 packages on x64# apt-get update\n\n\n# If not use the deb packages below but the repo above, at this point you can execute;# sudo apt-get install dellomsa \n\n\n#Grab OMSA and debian package (can be http): ONLY IF NOT USING REPO ABOVE #wget ftp://ftp.sara.nl/pub/outgoing/dell/binary-i386/dellomsa_5.5.0-5_i386.debwget ftp://ftp.sara.nl/pub/outgoing/dell/binary-amd64/dellomsa_5.5.0-5_amd64.debdpkg -i dellomsa_5.5.0-5_amd64.deb#At this point, run the following to update libraries when using either repo or deb pkg sudo ldconfig#For 32bit machine with library/dependencies missing;sudo apt-get install libstdc++5#May require: sudo apt-get install gcc-3.3-base#For 64 bit machine we may need 32bit libstdc++5 from here, forced install on 64 bit machine:wget http://ftp.de.debian.org/debian/pool/main/g/gcc-3.3/libstdc++5_3.3.6-20_i386.deb# http://ftp.de.debian.org/debian/pool/main/g/gcc-3.3/libstdc++5_3.3.6-20_amd64.deb# Mirrors;# /uploads/linux/dell/libstdc++5_3.3.6-20_i386.deb# /uploads/linux/dell/libstdc++5_3.3.6-20_amd64.debsudo dpkg --force-architecture -i libstdc++5_3.3.6-20_i386.deb#Some links maybe needed:#sudo ln -s /usr/lib/libstdc++.so.5 /lib/libstdc++.so.5#sudo ln -s /usr/lib/libstdc++.so.5 /lib32/libstdc++.so.5#sudo ln -s /usr/lib/libstdc++.so.5 /lib64/libstdc++.so.5#Pop this handy script into /etc/init.d (Thanks to James Kyle) and run it sudo wget -O /etc/init.d/dell_omsa.sh http://null.53bits.co.uk/uploads/linux/dell/dell_omsa.shsudo chmod +x /etc/init.d/dell_omsa.shsudo /etc/init.d/dell_omsa.sh\n\n\n# Enable SNMP in OMSA sudo /etc/init.d/dataeng enablesnmp# Make sure both are running sudo /etc/init.d/snmpd restartsudo /etc/init.d/dataeng startsudo /etc/init.d/dsm_om_connsvc startsudo /etc/init.d/dsm_om_shrsvc start #Download 32bit pam auth modules if on 64 (also, force inet4 sometimes):wget http://mirrors.kernel.org/ubuntu/pool/main/p/pam/libpam-modules_1.1.3-2ubuntu1_i386.deb --inet4-only# Newer versions:# http://mirrors.kernel.org/ubuntu/pool/main/p/pam/libpam-modules_1.1.3-6ubuntu1_amd64.deb# http://mirrors.kernel.org/ubuntu/pool/main/p/pam/libpam-modules_1.1.3-6ubuntu1_i386.debdpkg-deb -x libpam-modules_1.1.3-2ubuntu1_i386.deb ./cd ./lib/i386-linux-gnu/securitysudo cp pam_unix.so /lib32/securitysudo cp pam_nologin.so /lib32/securitysudo ldconfig \n\n\n# Look at 32bit/64bit mixing authentication errors here:# https://oss.trac.surfsara.nl/omsa_2_deb/ticket/37#Restart sudo /etc/init.d/dsm_om_connsvc restart#OMSA seems to bind to IPv6 as shown by netstat -nl, check in /opt/dell/srvadmin/iws/config/iws.ini# tcp6 :::1311#browser: https://[2a01:420:9:0:213:72ff:fe53:2c05]:1311# omreport and omconfig can now be used for remotely configuring and reporting on the Perc RAID controller(s);#sudo omreport storage controller#sudo omreport storage controller controller=0# consistency check on vdisk 0 on controller 0#sudo omconfig storage vdisk action=checkconsistency controller=0 vdisk=0 #sudo omreport storage controller No controllers found#try;# sudo apt-get install dkms# sudo service dataeng restart#Physical disks sudo omreport storage pdisk controller=0# If you get the error:# omreport: error while loading shared libraries: libpam.so.0# sudo apt-get install libpam0g:i386\nMegaCLI/MegaRAID\n# might need the following if not installed\nsudo apt-get install alien unzip\n# might also need\nsudo apt-get install sysfsutils\n\n#Download link\nwget http://www.lsi.com/downloads/Public/MegaRAID%20Common%20Files/8.02.16_MegaCLI.zip\n# mirror:\n# /uploads/linux/dell/8.02.16_MegaCLI.zip\nunzip 8.02.16_MegaCLI.zip\ncd LINUX\nunzip MegaCliLin.zip\n# Install the bundled libraries;\nsudo alien -iv Lib_Utils-1.00-09.noarch.rpm\n\n# Upack;\nrpm2cpio MegaCli-8.02.16-1.i386.rpm | cpio -dimv\n\n# copy files to /opt\nsudo mv opt/MegaRAID/ /opt/\ncd /opt/MegaRAID/MegaCli/\nsudo ./MegaCli -AdpEventLog -GetEvents -aAll\n\n# These maybe are needed if OMSA isn't installed as above and cary change for 32/64bit systems;\n# sudo apt-get install -f lib32ncurses5\n# sudo apt-get install -f lib32ncurses5\n# sudo apt-get install -f libncurses5\n# sudo apt-get install gcc\n\n# Install reference:\n# http://staff.blog.ui.ac.id/jp/2010/10/07/installing-megacli-in-debian-based-system/\n\n# Example commands:\n# http://tools.rapidsoft.de/perc/perc-cheat-sheet.html\n\n# Controller information\nMegaCli -AdpAllInfo -aALL \nMegaCli -CfgDsply -aALL \nMegaCli -AdpEventLog -GetEvents -f events.log -aALL && cat events.log\n\n# Enclosure information\nMegaCli -EncInfo -aALL\n\n# Virtual drive information\nMegaCli -LDInfo -Lall -aALL\n\n# Physical drive information\nMegaCli -PDList -aALL \nMegaCli -PDInfo -PhysDrv [E:S] -aALL\n\n# Battery backup information\nMegaCli -AdpBbuCmd -aALL\n\n# Check State of Health\n./MegaCli adpbbucmd getbbustatus a0 | grep SOH\n\n\n# Updating firmware\nmkdir fw\ncd fw\nwget \"http://ftp.us.dell.com/SAS-RAID/DELL_PERC-6-I-INTEGRATED_A12_R278433.exe\"\nunzip DELL_PERC-6-I-INTEGRATED_A12_R278433.exe\nMegaCli64 -adpfwflash -f FW952II.rom -a0\n# That is actually a Windows firmware update file but it has the same rom in side it;\n\n\n./MegaCli64 -adpfwflash -f FW1371iI.rom -a0\n                                     \nAdapter 0: PERC 6/i Integrated\nVendor ID: 0x1000, Device ID: 0x0060\n\nPackage version on the controller: 6.1.1-0047 \nPackage version of the image file: 6.3.1-0003 \nDownload Completed.     \nFlashing image to adapter...\nAdapter 0: Flash Completed.\n\nExit Code: 0x00\n\n# For older perc cards like perc 4's, try lsiutil, or use omreport above# wget http://www.lsi.com/downloads/Public/Obsolete/Obsolete%20Common%20Files/LSIUtil_1.62.zip# mirror: wget  /uploads/linux/dell/LSIUtil_1.62.zip\nPower Monitoring\n# omreport chassis pwrsupplies\nPower Supplies Information\n\nPower Supply Redundancy\nRedundancy Status : Full\n\nIndividual Power Supply Elements\nIndex                    : 0\nStatus                   : Ok\nLocation                 : PS 1 Status\nType                     : AC\nRated Input Wattage      : [No Value]\nMaximum Output Wattage   : 750 W\nOnline Status            : Presence Detected\nPower Monitoring Capable : Yes\n\nIndex                    : 1\nStatus                   : Ok\nLocation                 : PS 2 Status\nType                     : AC\nRated Input Wattage      : [No Value]\nMaximum Output Wattage   : 750 W\nOnline Status            : Presence Detected\nPower Monitoring Capable : Yes\n\n\n# omreport chassis pwrmonitoring\nPower Consumption Information\n\nPower Consumption\nIndex             : 2\nStatus            : Ok\nProbe Name        : System Board System Level\nReading           : 244 W\nWarning Threshold : 916 W\nFailure Threshold : 964 W\n\nAmperage\nPS 1 Current 1 : 0.8 A\nPS 2 Current 2 : 1.0 A\n\nPower Tracking Statistics\nStatistic               : Energy Consumption\nMeasurement Start Time  : Tue May  6 21:58:38 2008\nMeasurement Finish Time : Wed Feb  1 13:55:29 2012\nReading                 : 1008.6 KWh\n\nStatistic              : System Peak Power\nMeasurement Start Time : Tue May  6 21:58:38 2008\nPeak Time              : Sat Mar  6 23:42:11 2010\nPeak Reading           : 407 W\n\nStatistic              : System Peak Amperage\nMeasurement Start Time : Tue May  6 21:58:38 2008\nPeak Time              : Sat Mar  6 23:23:27 2010\nPeak Reading           : 3.3 A\nIPMI Commands\n#Graceful restartipmitool -I lanplus -H 172.22.0.150 -U root -P password chassis power soft\n#Boot from CD\nipmitool -I lan -H 172.22.0.150  -U root -P password chassis bootdev cdrom\n#Enable Serial-over-Labelipmitool -I lanplus -H 172.22.0.150 -U root -P password sol activate# Dont forget to enable Bios > Serial Communication > \"On with Console Redirection via COM2\" and \"Redirect after Reboot\" \nRAID Monitoring\n# List storage controllers\n# omreport storage controller\n\n Controller  PERC 4e/Di (Embedded)\n\nControllers\nID                                : 0\nStatus                            : Non-Critical\nName                              : PERC 4e/Di\nSlot ID                           : Embedded\nState                             : Degraded\nFirmware Version                  : 522A\nMinimum Required Firmware Version : 522D\nDriver Version                    : Not Applicable\nMinimum Required Driver Version   : Not Applicable\nNumber of Connectors              : 2\nRebuild Rate                      : 30%\nBGI Rate                          : Not Applicable\nCheck Consistency Rate            : Not Applicable\nReconstruct Rate                  : Not Applicable\nAlarm State                       : Not Applicable\nCluster Mode                      : Not Applicable\nSCSI Initiator ID                 : 7\nCache Memory Size                 : 256 MB\nPatrol Read Mode                  : Auto\nPatrol Read State                 : Stopped\nPatrol Read Rate                  : Not Applicable\nPatrol Read Iterations            : 11526\n\n\n\n# List details from a specific controller\n# omreport storage controller controller=0\n\n Controller  PERC 4e/Di (Embedded)\n\nControllers\nID                                : 0\nStatus                            : Non-Critical\nName                              : PERC 4e/Di\nSlot ID                           : Embedded\nState                             : Degraded\nFirmware Version                  : 522A\nMinimum Required Firmware Version : 522D\nDriver Version                    : Not Applicable\nMinimum Required Driver Version   : Not Applicable\nNumber of Connectors              : 2\nRebuild Rate                      : 30%\nBGI Rate                          : Not Applicable\nCheck Consistency Rate            : Not Applicable\nReconstruct Rate                  : Not Applicable\nAlarm State                       : Not Applicable\nCluster Mode                      : Not Applicable\nSCSI Initiator ID                 : 7\nCache Memory Size                 : 256 MB\nPatrol Read Mode                  : Auto\nPatrol Read State                 : Stopped\nPatrol Read Rate                  : Not Applicable\nPatrol Read Iterations            : 11526\n\nConnectors\nID             : 0\nStatus         : Ok\nName           : Connector 0\nState          : Ready\nConnector Type : SCSI Channel RAID Mode\nTermination    : Wide Terminated\nSCSI Rate      : Ultra 320M SCSI\n\nID             : 1\nStatus         : Ok\nName           : Connector 1\nState          : Ready\nConnector Type : SCSI Channel Non-RAID Mode\nTermination    : Wide Terminated\nSCSI Rate      : Ultra 320M SCSI\n\nVirtual Disks\nID                  : 0\nStatus              : Ok\nName                : Virtual Disk 0\nState               : Ready\nProgress            : Not Applicable\nLayout              : RAID-5\nSize                : 136.48 GB (146548981760 bytes)\nDevice Name         : /dev/sda\nType                : SCSI\nRead Policy         : Adaptive Read Ahead\nWrite Policy        : Write Back\nCache Policy        : Direct I/O\nStripe Element Size : 64 KB\nDisk Cache Policy   : \n\nPhysical Disks\nID                        : 0:0\nStatus                    : Ok\nName                      : Physical Disk 0:0\nState                     : Online\nFailure Predicted         : No\nProgress                  : Not Applicable\nType                      : SCSI\nCapacity                  : 68.24 GB (73274490880 bytes)\nUsed RAID Disk Space      : 68.24 GB (73274490880 bytes)\nAvailable RAID Disk Space : 0.00 GB (0 bytes)\nHot Spare                 : No\nVendor ID                 : SEAGATE \nProduct ID                : ST373207LC      \nRevision                  : D701\nSerial No.                : 3KT2NQDN\nNegotiated Speed          : 320\nCapable Speed             : 320\nManufacture Day           : Not Available\nManufacture Week          : Not Available\nManufacture Year          : Not Available\nSAS Address               : Not Available\n\nID                        : 0:1\nStatus                    : Ok\nName                      : Physical Disk 0:1\nState                     : Online\nFailure Predicted         : No\nProgress                  : Not Applicable\nType                      : SCSI\nCapacity                  : 68.24 GB (73274490880 bytes)\nUsed RAID Disk Space      : 68.24 GB (73274490880 bytes)\nAvailable RAID Disk Space : 0.00 GB (0 bytes)\nHot Spare                 : No\nVendor ID                 : SEAGATE \nProduct ID                : ST373207LC      \nRevision                  : D703\nSerial No.                : 3KT3ELLN\nNegotiated Speed          : 320\nCapable Speed             : 320\nManufacture Day           : Not Available\nManufacture Week          : Not Available\nManufacture Year          : Not Available\nSAS Address               : Not Available\n\nID                        : 0:2\nStatus                    : Ok\nName                      : Physical Disk 0:2\nState                     : Online\nFailure Predicted         : No\nProgress                  : Not Applicable\nType                      : SCSI\nCapacity                  : 68.24 GB (73274490880 bytes)\nUsed RAID Disk Space      : 68.24 GB (73274490880 bytes)\nAvailable RAID Disk Space : 0.00 GB (0 bytes)\nHot Spare                 : No\nVendor ID                 : SEAGATE \nProduct ID                : ST373207LC      \nRevision                  : D701\nSerial No.                : 3KT2PYK8\nNegotiated Speed          : 320\nCapable Speed             : 320\nManufacture Day           : Not Available\nManufacture Week          : Not Available\nManufacture Year          : Not Available\nSAS Address               : Not Available\n\nBattery\nID                        : 0\nStatus                    : Ok\nName                      : Battery 0\nState                     : Ready\nRecharge Count            : 1\nMax Recharge Count        : 1100\nPredicted Capacity Status : Not Applicable\nLearn State               : Not Applicable\nNext Learn Time           : Not Applicable\nMaximum Learn Delay       : Not Applicable\n\nEnclosure(s)\nID                    : 0\nStatus                : Ok\nName                  : Backplane\nState                 : Ready\nConnector             : 0\nTarget ID             : 6\nConfiguration         : Not Applicable\nFirmware Version      : 1.0 \nService Tag           : 54S1063\nAsset Tag             : Not Applicable\nAsset Name            : Not Applicable\nBackplane Part Number : Not Applicable\nSplit Bus Part Number : Not Applicable\nEnclosure Part Number : Not Applicable\nSAS Address           : Not Applicable\nEnclosure Alarm       : Not Applicable\n\n\n\n# List physical disks on controller 0\n# omreport storage pdisk controller=0\n\nList of Physical Disks on Controller PERC 4e/Di (Embedded)\n\nController PERC 4e/Di (Embedded)\nID                        : 0:0\nStatus                    : Ok\nName                      : Physical Disk 0:0\nState                     : Online\nFailure Predicted         : No\nProgress                  : Not Applicable\nType                      : SCSI\nCapacity                  : 68.24 GB (73274490880 bytes)\nUsed RAID Disk Space      : 68.24 GB (73274490880 bytes)\nAvailable RAID Disk Space : 0.00 GB (0 bytes)\nHot Spare                 : No\nVendor ID                 : SEAGATE \nProduct ID                : ST373207LC      \nRevision                  : D701\nSerial No.                : 3KT2NQDN\nNegotiated Speed          : 320\nCapable Speed             : 320\nManufacture Day           : Not Available\nManufacture Week          : Not Available\nManufacture Year          : Not Available\nSAS Address               : Not Available\n\n^ Top\nPrevious page:\u00a0Check Dell RAID, Disks and Battery via SNMPNext page:\u00a0Dell RAID Notes\n"}, {"name": "Windows \u00faprava \u0161\u0148upan\u00ed", "date": "20.1.2021", "author": "admin", "url": "https://blog.iservery.com/2021/01/20/windows-uprava-snupani/", "content": "\nhttps://www.chip.cz/novinky/bezpecnejsi-windows-10-nastroj-wpd-vypne-cmuchani-operacniho-systemu/\n"}, {"name": "Cisco", "date": "30.12.2020", "author": "admin", "url": "https://blog.iservery.com/2020/12/30/cisco/", "content": "\nZdroj firmware\nhttps://cdn.cisko.dk/Misc/\n"}, {"name": "Manualy Proxmox", "date": "29.12.2020", "author": "admin", "url": "https://blog.iservery.com/2020/12/29/manualy-proxmox/", "content": "\nhttps://manualzz.com/doc/33630331/mastering-proxmox\n"}, {"name": "Dell R710 bios update", "date": "29.12.2020", "author": "admin", "url": "https://blog.iservery.com/2020/12/29/dell-r710-bios-update/", "content": "\nDownload R710-060600C.exe\nhttps://www.dell.com/support/home/cs-cz/drivers/driversdetails?driverid=0f4yy\nnainstalovat freedos z https://www.freedos.org/ \nna usb boot  pouzit treba rufus  https://rufus.ie/\npustit soubor s parametrem R710-060600C.exe /forcetype\n"}, {"name": "Jak na tvorbu web prezentace", "date": "15.12.2020", "author": "admin", "url": "https://blog.iservery.com/2020/12/15/jak-na-tvorbu-web-prezentace/", "content": "\nDohledov\u00fd syst\u00e9m \u2013 \u017ee to po\u0159\u00e1d fungujenap\u0159 zabbix\nStatistika anal\u00fdza z\u00e1kazn\u00edku a jejich pohyb na webugoogle analyticshttps://github.com/bitnami/bitnami-docker-matomo nebo matomo GPL mnohem propracovan\u011bj\u0161\u00ed\nUX jak na u\u017eivatelsk\u00e9 rozhran\u00ed ilincev.com\npro inspiraci nic nedelaji ale penize sbiraji https://www.xixoio.com/o-nas/\n"}, {"name": "Zoneminder OpenAlpr", "date": "13.12.2020", "author": "admin", "url": "https://blog.iservery.com/2020/12/13/zoneminder-openalpr/", "content": "\nhttps://github.com/pliablepixels/zoneminder-docker/blob/master/zmeventnotification/objectconfig.ini\nhttps://groups.google.com/g/openalpr?pli=1\nhttps://zoneminder.miraheze.org/wiki/Zoneminder_with_plate_recognition\ndocker build -t openalpr https://github.com/mandza/openalpr.git\nwget https://rzauto.cz/wp-content/uploads/2020/04/0688_6.jpg\ndocker run -it \u2013rm -v $(pwd):/data:ro openalpr -c eu 0699_6.jpg\n"}, {"name": "kuchyn\u011b a lo\u017enice", "date": "5.11.2020", "author": "admin", "url": "https://blog.iservery.com/2020/11/05/kolecka-na-bar-kuchyne/", "content": "\nhttps://lednice.heureka.cz/samsung-brb260034ww/specifikace/#section\nhttps://mycky-nadobi.heureka.cz/aeg-fsb53927z/ koupeno\nhttps://www.kolapirkl.cz/nerez-nerez-nerez prumer 30mm\nhttps://www.kolapirkl.cz/kulickova-jednotka-alwayse-euro-530-0-14\nhttps://www.sezamcz.cz/stavece-dveri\nLo\u017enice postele\nhttps://www.spime.cz/postel-z-masivu-gita\nhttps://www.spime.cz/postel-z-masivu-capri-dub-drasany\n\n"}, {"name": "Tastoma Sonoff roleta a autokonfigurace HA", "date": "1.11.2020", "author": "admin", "url": "https://blog.iservery.com/2020/11/01/tastoma-sonoff-roleta-a-autokonfigurace-ha/", "content": "\nNastaveni autokonfigurace pro firmware Tastoma  \u2013 SetOption19 1P\u0159epnut\u00ed na rolety, \u017ealuzie pro spina\u010de se 2 rele \u2013  SetOption80 1\nhttps://tasmota.github.io/docs/Blinds-and-Shutters/\nz\u00e1kladn\u00ed nastaven\u00ed pro zaluzie somfy\nSetOption19 1SetOption80 1ShutterCloseDuration 45; ShutterOpenDuration 45;backlog interlock off; shutterrelay1 1; shuttersetclose1; interlock 1,2; interlock on; shutterrelay1 1; shutteropen1;\n\npravidla alias rules\nrule2 onrule2 on button1#state=3 do backlog power1 off; power2 off;power1 on; delay 1; power1 off; endon\nsetoption32 4 # long press after 4s\nswitchmode 5 # long press button\nrun as batch one line\nbacklog rule2 on; setoption32 4; switchmode 5; \nrule2 on button1#state=3 do backlog power1 off; power2 off;power1 on; delay 1; power1 off; endon; \n\nposlani mqtt publish \nbacklog rule2 on; setoption32 5; switchmode 5; \nrule2 on button1#state=3 do Publish 4relay/cmd GPIO,12,1 endon\n\n"}, {"name": "Festo \u2013 Motion terminal VTEM \u2013 z\u00e1klady", "date": "22.10.2020", "author": "admin", "url": "https://blog.iservery.com/2020/10/22/festo-motion-terminal-vtem-zaklady/", "content": "\nfesto.cz nutn\u00e9 utility\nVyhledavani a nastaveni IP adres termin\u00e1lu \u2013 SW \u2013  FFT  Festo field device tools odkaz\nCodesys provided by Festo odkaz \ndo Codesysu pridat knihovny a package pro ovladani modulu \n\u2013 VTEM = blok s ventily \u2013 ovlada\u010d zde\n\u2013 package pro CPX-CEC = plc procesor  \u2013 ovlada\u010d zde\nPo nastartovani CODESYSU \u2013 instalace nahrat CPX-CEC package a knihovnu VTEM\n\n"}, {"name": "N\u00e1kup HW ze zahrani\u010d\u00ed", "date": "14.10.2020", "author": "admin", "url": "https://blog.iservery.com/2020/10/14/nakup-hw-ze-zahranici/", "content": "\nhttps://www.2nd-source.de\nSwitche kably karty Server\nBrocade 1.0m 40G QSFP\nBrocade 24x RJ-45 10/100/1000Base-T 8x SFP 4x QSFP Switch ICX6610-24P\n\n\n"}, {"name": "Esp32 BLE Xiaomi temperature humidity sensor LYWSD03MMC", "date": "14.5.2020", "author": "admin", "url": "https://blog.iservery.com/2020/05/14/esp32-ble-xiaomi-temperature-humidity-sensor-lywsd03mmc/", "content": "\nhttps://github.com/AnthonyKNorman/Xiaomi_LYWSD03MMC_for_HA\n"}, {"name": "oprava site IT4", "date": "6.5.2020", "author": "admin", "url": "https://blog.iservery.com/2020/05/06/oprava-site-it4/", "content": "\nna routeru, kde je NATaccess-list 1 permit 192.168.0.0 0.0.252.0\nzmeneno na \naccess-list 1 permit 192.168.0.0 0.0.3.255\nip route 0.0.0.0 0.0.0.0 213.155.255.1\nzmeneno na \nip route 0.0.0.0 0.0.0.0 213.155.225.1\nopraveno ospf,dhcp,acl,lacp,sshSt\u00e1hnout\nospf,dhcp,acl,lacp,sshSt\u00e1hnout\n"}, {"name": "Instalace wordpress na Ubuntu 18.04", "date": "27.4.2020", "author": "admin", "url": "https://blog.iservery.com/2020/04/27/instalace-wordpress-na-ubuntu-18-04/", "content": "\nmusite si nainstalovat phpmyadmin a mysql server odkaz\ndale pokracovat podle videa\n\n\nPlease enable JavaScriptplay-sharp-fill\n\n\n\nLinkEmbedCopy and paste this HTML code into your webpage to embed.\n\n\n"}, {"name": "Uk\u00e1zkov\u00fd p\u0159\u00edklad maturitn\u00ed pr\u00e1ce Cisco \u2013 dynamick\u00fd routing", "date": "25.4.2020", "author": "admin", "url": "https://blog.iservery.com/2020/04/25/ukazkovy-priklad-maturitni-prace-cisco-dynamicky-routing/", "content": "\nPraktick\u00fd p\u0159\u00edklad k p\u0159\u00edprav\u011b odkazVidea k p\u0159\u00edkladu \n\nPlease enable JavaScriptplay-sharp-fill\n\n\n\nLinkEmbedCopy and paste this HTML code into your webpage to embed.\n\n\n\nPlease enable JavaScriptplay-sharp-fill\n\n\n\nLinkEmbedCopy and paste this HTML code into your webpage to embed.\n\nSpln\u011bn\u00fd p\u0159\u00edklad odkaz\n\n"}, {"name": "Uk\u00e1zka maturitn\u00edho p\u0159\u00edkladu Cisco \u2013 statick\u00fd routing", "date": "25.4.2020", "author": "admin", "url": "https://blog.iservery.com/2020/04/25/ukazka-maturitniho-prikladu-cisco-staticky-routing/", "content": "\nZad\u00e1n\u00ed pr\u00e1ce \u2013 odkazVy\u0159e\u0161en\u00fd p\u0159\u00edklad \u2013 odkaz\u0158e\u0161en\u00ed video\n\nPlease enable JavaScriptplay-sharp-fill\n\n\n\nLinkEmbedCopy and paste this HTML code into your webpage to embed.\n\n\n\n"}, {"name": "Micropython  + LoRa WAN", "date": "6.4.2020", "author": "admin", "url": "https://blog.iservery.com/2020/04/06/micropython-lora-wan/", "content": "\nhttps://lemariva.com/blog/2020/03/m5stack-atom-lorawan-node-running-micropython\nhttps://github.com/hiveeyes/terkin-datalogger\nhttps://ttnmapper.org/alpha-shapes/\n"}, {"name": "Instalace Micropython a Vscode", "date": "6.4.2020", "author": "admin", "url": "https://blog.iservery.com/2020/04/06/instalace-micropython-a-vscode/", "content": "\nInstalace vscode https://linuxize.com/post/how-to-install-visual-studio-code-on-ubuntu-18-04/\nsudo apt update\nsudo apt install software-properties-common apt-transport-https wget\nwget -q https://packages.microsoft.com/keys/microsoft.asc -O- | sudo apt-key add \u2013\nsudo add-apt-repository \u201edeb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\u201c\nsudo apt update\nsudo apt install code\nInstalace pluginu vscode \u2013  https://lemariva.com/blog/2018/12/micropython-visual-studio-code-as-ide instalace pymakr instalace esptools.py node.js \nInstalace micropython na device esp32 nebo esp8266\nhttps://lemariva.com/blog/2020/03/tutorial-getting-started-micropython-v20\nn\u00e1sledn\u011b p\u0159ipojen\u00ed na konzoli esp32\n"}, {"name": "Video Cisco ssh + ACL", "date": "10.2.2020", "author": "admin", "url": "https://blog.iservery.com/2020/02/10/video-cisco-ssh-acl/", "content": "\n\n\nPlease enable JavaScriptplay-sharp-fill\n\n\n\nLinkEmbedCopy and paste this HTML code into your webpage to embed.\n\n\nCisco Lekce 1\n\n"}]